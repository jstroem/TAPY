\chapter{Handling functions}
In this chapter we start with motivating why it is necessary to have three objects on the heap for each function declaration. Consider the listing \ref{code:FunctionPropertyExample}. As mentioned we create a function object on the heap for each function declaration. This object is necessary as functions are themselves objects, just like in e.g. JavaScript. For instance we can set a property on a function object as line 2 illustrates.

\begin{listing}[H]
	\begin{minted}[linenos]{python}
def a(): pass
a.prop = 42
	\end{minted}
\caption{Property on function object}\label{code:FunctionPropertyExample}
\end{listing}

Thus for this concrete example we map the property \inlinecode{prop} to the integer 42 on the object of the function.

Another relevant thing with regards to function objects on the heap, is that Python has a built in method \inlinecode{\_\_call\_\_} on each function. This method is a function wrapper of the function itself; calling it will result in calling the function itself. We therefore map the property \inlinecode{\_\_call\_\_} on the function object to its function wrapper object. The following illustrates how each newly declared function has this method:

\begin{listing}[H]
	\begin{minted}[linenos]{python}
def a():
	print "a"
a() // "a"
a.__call__ # <method-wrapper '__call__' of function object at ...> 
a.__call__() # "a"
	\end{minted}
\caption{On a newly declared function the \_\_call\_\_ property is set to a built in method wrapper.}\label{code:printFunctionExample}
\end{listing}

It is important to distinguish between the object of the function, and the function object, since \inlinecode{\_\_call\_\_} is not just a reference to the object of the function, as illustrated below:

\begin{listing}[H]
	\begin{minted}[linenos]{python}
def a(): 
	pass

# TypeError: 'method-wrapper' object has only read-only attributes
a.__call__.prop = 10
	\end{minted}
\caption{Function object and \_\_call\_\_ example}\label{code:callPropertyExample}
\end{listing}

Finally, the scope object of a function in listing \ref{code:callPropertyExample} is necessary as local variables inside a function should not be set as properties on the object of the function.
In the analysis there is only created one scope object on the heap, this is an abstraction since when running a function in Python a new scope object would be created every time a function is invoked. The abstraction makes it possible to create the scope object when the function is declared instead of doing it when a function is invoked.

\begin{listing}[H]
	\begin{minted}[linenos]{python}
def a(): 
	x = 42
a.x # AttributeError
	\end{minted}
\caption{Function object and \_\_call\_\_ example}\label{code:callPropertyExample}
\end{listing}


\section{Calling functions and updating the call graph}
We need to describe what we do when functions are called, how the call graph is populated, and how we fix parameter passing\todo{Fixme}.
When converting a function declaration from the AST representation into the CFG representation, stack space is allocated for the default arguments. Default arguments is only evaluated once when the function is declared and when normal arguments is passed into the function, registers to the default arguments is used if the normal argument on that position is absent.

\section{Strong or weak updates of local variables}
Since only one scope object is created for each function, there is a big potential for presition loss because soundness would dictate that all writes to local variable are modeled as weak updates, i.e. least upper bound between current value and the new value. This is due to the fact that a scobe object of a perticular function might exist in more than one instance at runtime, as it is the case with recursive functions. An examble would be if, at runtime, one instance of a scope object for a particular function has some variable z to be 5 while another has it to be 10, then the analysis would need to emulate both these values at the same time in the scope object. Ths can be achieved by taking the least upper bound of the two values, but this has potential to grealt reduce the accuraru of the analysis since the only upperbound possible in our value lattice is the any integer element.

This problem only arises in the presence of recursive functions, so to enable the analysis to do strong updates a flag is introduced to tell the analysis if it can assume no recursive functions. This is a very fast to implement and allowed more time to focus on the magic methods, which on its own doesn't introduce any recursion.

Alternativly to this fix, the call graph could be inspected for lopps indicating recusive functions. However since the call graph is built dynamicly as a part of the analysis a recursive function might not appear as recursive at the current iteration possibly leading to strong updates, thus special care is needed when functions are labeled recursive.

A different approached could be to use the recency abstraction as introduced by \cite{recency}. This could work very well because all writes to local objects at runtime happen to the lastest instantiated stackframe, which is exactly the situation recency abstraction enables strong updates over weak updates.

\section{Further work with functions}
In Python it is possible to unfold e.g. a dictionary to the arguments of a function:

\begin{listing}[H]
	\begin{minted}[linenos]{python}
def foo(bar, baz):
	return bar + baz
params = { 'bar': 'bar', 'baz': 'baz' }
foo(*params) # 'barbaz'
	\end{minted}
\caption{Unfolding of a dictionary to the parameters a function.}\label{code:UnfoldDictFunctionExample}
\end{listing}

Currently, our analysis does not support this kind of parameter passing. Also, it does not support the special \inlinecode{**args} parameter that collects all of the superfluous arguments in a list, quite similar to the \inlinecode{arguments} object that is available inside functions in JavaScript.
