\chapter{Handling functions}
\label{Functions}
In this chapter we start with motivating why it is necessary to have three objects on the heap for each function declaration. Consider the below listing \ref{code:FunctionPropertyExample}. As mentioned we create a function object on the heap for each function declaration. This object is necessary as functions are themselves objects, just like in e.g. JavaScript. For instance we can set a property on a function object as line 2 illustrates.

\begin{listing}[H]
	\begin{minted}[linenos]{python}
def a(): pass
a.prop = 42
	\end{minted}
\caption{Property on function object}\label{code:FunctionPropertyExample}
\end{listing}

Thus for this concrete example we map the property \inlinecode{prop} to the integer 42 on the object of the function.

Another relevant thing with regards to function objects on the heap, is that Python has a built in method \inlinecode{\_\_call\_\_} on each function. This method is a function wrapper of the function itself; calling it will result in calling the function itself. We therefore map the property \inlinecode{\_\_call\_\_} on the function object to its function wrapper object. The following illustrates how each newly declared function has this method:

\begin{listing}[H]
	\begin{minted}[linenos]{python}
def a():
	print "a"
a() // "a"
a.__call__ # <method-wrapper '__call__' of function object at ...> 
a.__call__() # "a"
	\end{minted}
\caption{On a newly declared function the \_\_call\_\_ property is set to a built in method wrapper.}\label{code:printFunctionExample}
\end{listing}

It is important to distinguish between the object of the function, and the function object, since \inlinecode{\_\_call\_\_} is not just a reference to the object of the function, as illustrated below:

\begin{listing}[H]
	\begin{minted}[linenos]{python}
def a(): 
	pass

# TypeError: 'method-wrapper' object has only read-only attributes
a.__call__.prop = 10
	\end{minted}
\caption{Function object and \_\_call\_\_ example}\label{code:callPropertyExample}
\end{listing}

Finally, the scope object of a function in listing \ref{code:callPropertyExample} is necessary as local variables inside a function should not be set as properties on the object of the function.
In the analysis there is only created one scope object on the heap, this is an abstraction since when running a function in Python a new scope object would be created every time a function is invoked. The abstraction makes it possible to create the scope object when the function is declared instead of doing it when a function is invoked.

\begin{listing}[H]
	\begin{minted}[linenos]{python}
def a(): 
	x = 42
a.x # AttributeError
	\end{minted}
\caption{Function object and \_\_call\_\_ example}\label{code:callPropertyExample}
\end{listing}


\section{Calling functions and updating the call graph}
A call is represented by a \inlinecode{CallNode} (and \inlinecode{AfterCallNode}) in the CFG.

If the value being called is a function $f(p_1, ..., p_2)$, we first look up the scope object of the function $f$ on the heap. We need the scope object of the function in order to handle parameter passing: For each parameter $p_i$ of $f$ we set $p_i$ as an attribute on the scope object of $f$ to the value of $a_i$, the i'th supplied argument. This implies that reading the variable $a_i$ inside the function will yield the supplied argument.

However, if the function is called more than once, reading the i'th argument inside the function will result in the least upper bound of all supplied i'th arguments because we have no context sensitivity! The following example illustrates this:

\begin{listing}[H]
	\begin{minted}[linenos]{python}
def f(p1): 
	return p1
f(10)
x = f(20) # x becomes the top element of the integer lattice, not 20
	\end{minted}
\caption{An example illustrating the consequence of not having context sensitivity.}
\end{listing}

Next, we update the call graph accordingly by inserting call edges from the call node to the entry node of $f$ and from the exit node of $f$ to the after call node. This implies that the work list of our fixed point algorithm will be updated with the entry node of $f$, such that it will be reevaluated.

At the \inlinecode{FunctionEntryNode} we change the runtime scope chain to the static scope chain of the function, which can be found on the function object on the heap (see the $Object$ lattice).

When the fixed point algorithm have processed the function the after call node can read the return value of the function (which we store in a special constant register), and store it on the stack.

\section{Strong or weak updates of local variables}
Since only one scope object is created for each function, there is a big potential for precision loss because soundness dictates that all writes to local variables are modeled as weak updates, i.e. least upper bound between current value and the new value. This is due to the fact that a scope object of a particular function might exist in more than one instance at runtime, as it is the case with recursive functions. An example would be if, at runtime, one instance of a scope object for a particular function has some variable \inlinecode{z} to be 5 while another has it to be 10, then the analysis would need to emulate both these values at the same time in the scope object. This can be achieved by taking the least upper bound of the two values, but this has potential to greatly reduce the accuracy of the analysis since the only upper bound possible in our value lattice is the any integer element.

This problem only arises in the presence of recursive functions, so to enable the analysis to do strong updates a flag is introduced to tell the analysis if it can assume no recursive functions. This is a very fast to implement and allowed more time to focus on the magic methods, which on its own doesn't introduce any recursion.

Alternatively to this fix, the call graph could be inspected for loops indicating recursive functions. However since the call graph is built dynamically as a part of the analysis a recursive function might not appear as recursive at the current iteration possibly leading to strong updates, thus special care is needed when functions are labeled recursive.

A different approach could be to use the recency abstraction as introduced by \cite{recency}. This could work very well because all writes to local objects at runtime happen to the latest instantiated stack frame, which is exactly the situation in which recency abstraction enables strong updates over weak updates.

When converting a function declaration from the AST representation into the CFG representation, stack space is allocated for the default arguments. Default arguments is only evaluated once when the function is declared and when normal arguments is passed into the function, registers to the default arguments is used if the normal argument on that position is absent.

\section{Further work with functions}
In Python it is possible to unfold e.g. a dictionary to the arguments of a function:

\begin{listing}[H]
	\begin{minted}[linenos]{python}
def foo(bar, baz):
	return bar + baz
params = { 'bar': 'bar', 'baz': 'baz' }
foo(*params) # 'barbaz'
	\end{minted}
\caption{Unfolding of a dictionary to the parameters a function.}\label{code:UnfoldDictFunctionExample}
\end{listing}

Currently, our analysis does not support this kind of parameter passing. Also, it does not support the special \inlinecode{**args} parameter that collects all of the superfluous arguments in a list, quite similar to the \inlinecode{arguments} object that is available inside functions in JavaScript.